#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Parallel MySQL Slow Log Aggregator (robust v2)
- Memory-map + boundary scan; shards aligned to record starts
- Multiprocessing (--jobs N) to parse shards in parallel
- Exact p95 using numpy (memory is assumed OK) â€” keeps per-fingerprint durations
- Chinese headers by default (--lang zh), English via --lang en
- Robust to truncated logs: optional marking and stats
- "Loose start" mode: treat "# Query_time:" as a valid record start when "# Time:" is missing
- Prints detailed timing and counters with --stats (default on)
"""
import os, re, sys, argparse, mmap, math, hashlib, time
from multiprocessing import Pool, cpu_count
from collections import defaultdict
from datetime import datetime, timedelta, timezone

try:
    import numpy as np
    import pandas as pd
except Exception as e:
    print("This script requires numpy and pandas. pip install numpy pandas", file=sys.stderr)
    raise

# Elasticsearch integration (optional)
try:
    from elasticsearch import Elasticsearch
    from elasticsearch.helpers import bulk
    ES_AVAILABLE = True
except ImportError:
    ES_AVAILABLE = False

# ---------- Normalization helpers (Enhanced) ----------
# å¢å¼ºçš„MySQL Hintå¤„ç† - æ”¯æŒæ›´å¤šæ ¼å¼
re_mysql_hint_versioned = re.compile(r"/\*![0-9]{5}.*?\*/", flags=re.DOTALL)  # /*!40001 ... */
re_mysql_hint_simple = re.compile(r"/\*!(?![0-9]{5}).*?\*/", flags=re.DOTALL)   # /*!STRAIGHT_JOIN */ ç­‰
re_mysql_hint_executor = re.compile(r"/\*\+.*?\*/", flags=re.DOTALL)             # /*+ ... */ Oracleé£æ ¼
re_inline_comment = re.compile(r"(--[^\n]*$)", flags=re.MULTILINE)
re_block_comment = re.compile(r"/\*(?![!+]).*?\*/", flags=re.DOTALL)            # æ™®é€šæ³¨é‡Šï¼Œä¸åŒ…æ‹¬hint
re_string = re.compile(r"('([^'\\]|\\.)*'|\"([^\"\\]|\\.)*\")", flags=re.DOTALL)
re_numeric = re.compile(r"\b\d+(\.\d+)?\b")
re_in_list = re.compile(r"\bIN\s*\((?:[^()]*|\([^()]*\))*\)", flags=re.IGNORECASE)
re_whitespace = re.compile(r"\s+")

def normalize_sql(sql: str) -> str:
    """
    å¢å¼ºç‰ˆSQLè§„èŒƒåŒ–ï¼Œæ›´å½»åº•åœ°å¤„ç†MySQL Hintså’Œå„ç§æ³¨é‡Š
    """
    s = sql.strip()
    
    # æŒ‰é¡ºåºå¤„ç†å„ç§MySQL Hintsï¼ˆä»å…·ä½“åˆ°ä¸€èˆ¬ï¼‰
    s = re_mysql_hint_versioned.sub(" ", s)    # /*!40001 SQL_NO_CACHE */
    s = re_mysql_hint_simple.sub(" ", s)       # /*!STRAIGHT_JOIN */
    s = re_mysql_hint_executor.sub(" ", s)     # /*+ USE_INDEX(t1 idx1) */
    
    # å¤„ç†å…¶ä»–æ³¨é‡Š
    s = re_block_comment.sub(" ", s)           # /* æ™®é€šæ³¨é‡Š */
    s = re_inline_comment.sub(" ", s)          # -- è¡Œæ³¨é‡Š
    
    # å‚æ•°åŒ–å¤„ç†
    s = re_in_list.sub(" IN (?) ", s)
    s = re_string.sub("?", s)
    s = re_numeric.sub("?", s)
    
    # æ¸…ç†ç©ºç™½å’Œæ ¼å¼åŒ–
    s = re_whitespace.sub(" ", s)
    s = s.rstrip("; ").strip()
    s = s.lower()
    
    return s

def fingerprint(sql: str) -> str:
    return hashlib.md5(normalize_sql(sql).encode("utf-8")).hexdigest()

def extract_main_table(sql: str):
    m = re.search(r"\bfrom\s+([`\"\w\.\-]+)", sql, flags=re.IGNORECASE)
    if not m:
        m = re.search(r"\bupdate\s+([`\"\w\.\-]+)", sql, flags=re.IGNORECASE)
    if not m:
        m = re.search(r"\binto\s+([`\"\w\.\-]+)", sql, flags=re.IGNORECASE)
    return m.group(1).strip("`\"") if m else None

# ---------- Time filtering helpers (Expert Redesign) ----------
def parse_mysql_time(time_str: str) -> datetime:
    """
    Parse various MySQL slow log time formats to datetime object.
    Optimized for performance and robustness.
    """
    if not time_str:
        return None
    
    time_str = time_str.strip()
    
    # Fast path: ISO format (most common in modern MySQL)
    try:
        if 'T' in time_str and ('+' in time_str or 'Z' in time_str):
            return datetime.fromisoformat(time_str.replace('Z', '+00:00'))
    except:
        pass
    
    # Unix timestamp (fastest to parse)
    try:
        if time_str.isdigit() and len(time_str) == 10:
            return datetime.fromtimestamp(int(time_str), tz=timezone.utc)
    except:
        pass
    
    # MySQL short format with optimized parsing
    try:
        if '-' in time_str:
            return datetime.strptime(time_str[:19], '%Y-%m-%d %H:%M:%S').replace(tzinfo=timezone.utc)
        elif len(time_str.split()[0]) == 6:
            dt = datetime.strptime(time_str[:15], '%y%m%d %H:%M:%S')
            if dt.year < 1931:
                dt = dt.replace(year=dt.year + 100)
            return dt.replace(tzinfo=timezone.utc)
    except:
        pass
    
    return None

def calculate_time_range(days: int) -> tuple:
    """Calculate time range for filtering with timezone awareness."""
    now = datetime.now(timezone.utc)
    
    if days == 0:
        start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        end = now.replace(hour=23, minute=59, second=59, microsecond=999999)
    else:
        start = (now - timedelta(days=days)).replace(hour=0, minute=0, second=0, microsecond=0)
        end = now
    
    return start, end

def smart_time_range_check(file_path: str, time_range: tuple, max_sample_size: int = 10 * 1024 * 1024) -> dict:
    """
    Smart sampling to determine if file contains data in time range.
    Uses head/middle/tail sampling instead of full scan.
    Returns: {
        'has_data_in_range': bool,
        'estimated_coverage': float,  # 0.0-1.0
        'sample_times': [datetime, ...],
        'file_time_range': (start_dt, end_dt)
    }
    """
    if not time_range:
        return {'has_data_in_range': True, 'estimated_coverage': 1.0}
    
    try:
        file_size = os.path.getsize(file_path)
        # For small files, do a more thorough check
        sample_size = min(max_sample_size, file_size // 3)
        
        sample_times = []
        file_start_time = None
        file_end_time = None
        
        with open(file_path, 'rb') as f:
            # Sample from head, middle, tail
            positions = [0, max(0, file_size // 2 - sample_size // 2), max(0, file_size - sample_size)]
            
            for pos in positions:
                f.seek(pos)
                chunk = f.read(sample_size).decode('utf-8', errors='ignore')
                
                # Find time patterns
                for line in chunk.split('\n')[:200]:  # Limit lines per chunk
                    if line.startswith('# Time:'):
                        time_str = line.split('# Time:', 1)[1].strip()
                        dt = parse_mysql_time(time_str)
                        if dt:
                            sample_times.append(dt)
                            if file_start_time is None or dt < file_start_time:
                                file_start_time = dt
                            if file_end_time is None or dt > file_end_time:
                                file_end_time = dt
                    elif line.startswith('SET timestamp='):
                        m = re.search(r'SET timestamp=(\d+);', line)
                        if m:
                            dt = parse_mysql_time(m.group(1))
                            if dt:
                                sample_times.append(dt)
                                if file_start_time is None or dt < file_start_time:
                                    file_start_time = dt
                                if file_end_time is None or dt > file_end_time:
                                    file_end_time = dt
        
        if not sample_times:
            return {'has_data_in_range': True, 'estimated_coverage': 0.0, 'reason': 'no_timestamps_found'}
        
        # Check if any sample time is in range
        target_start, target_end = time_range
        has_data = any(target_start <= dt <= target_end for dt in sample_times)
        
        # æ›´æ™ºèƒ½çš„è¦†ç›–ç‡ä¼°ç®—
        coverage = 0.0
        coverage_type = "unknown"
        
        if file_start_time and file_end_time and has_data:
            # è®¡ç®—æ–‡ä»¶æ—¶é—´è·¨åº¦å’Œç›®æ ‡æ—¶é—´è·¨åº¦
            file_duration = (file_end_time - file_start_time).total_seconds()
            target_duration = (target_end - target_start).total_seconds()
            
            # è®¡ç®—é‡å éƒ¨åˆ†
            overlap_start = max(file_start_time, target_start)
            overlap_end = min(file_end_time, target_end)
            
            if overlap_start <= overlap_end:
                overlap_duration = (overlap_end - overlap_start).total_seconds()
                
                # æƒ…å†µ1ï¼šæ–‡ä»¶å®Œå…¨åœ¨ç›®æ ‡èŒƒå›´å†…
                if file_start_time >= target_start and file_end_time <= target_end:
                    coverage = 1.0  # æ–‡ä»¶æ•°æ®å®Œå…¨è¦†ç›–ï¼Œåªæ˜¯æ—¶é—´èŒƒå›´å°
                    coverage_type = "full_file_in_range"
                
                # æƒ…å†µ2ï¼šç›®æ ‡èŒƒå›´å®Œå…¨åœ¨æ–‡ä»¶å†…
                elif target_start >= file_start_time and target_end <= file_end_time:
                    coverage = 1.0  # ç›®æ ‡æ—¶é—´å®Œå…¨è¢«æ–‡ä»¶è¦†ç›–
                    coverage_type = "full_range_covered"
                
                # æƒ…å†µ3ï¼šéƒ¨åˆ†é‡å ï¼ŒæŒ‰å®é™…æ•°æ®å¯†åº¦ä¼°ç®—
                else:
                    # åŸºäºé‡å æ—¶é—´å æ–‡ä»¶æ—¶é—´çš„æ¯”ä¾‹æ¥ä¼°ç®—
                    if file_duration > 0:
                        file_overlap_ratio = overlap_duration / file_duration
                        # å¦‚æœé‡å éƒ¨åˆ†å æ–‡ä»¶æ—¶é—´çš„å¤§éƒ¨åˆ†ï¼Œè®¤ä¸ºè¦†ç›–ç‡è¾ƒé«˜
                        if file_overlap_ratio > 0.8:
                            coverage = 0.9  # å¤§éƒ¨åˆ†æ–‡ä»¶æ•°æ®åœ¨èŒƒå›´å†…
                            coverage_type = "mostly_covered"
                        elif file_overlap_ratio > 0.5:
                            coverage = 0.7  # ä¸€åŠä»¥ä¸Šæ–‡ä»¶æ•°æ®åœ¨èŒƒå›´å†…
                            coverage_type = "partially_covered"
                        else:
                            coverage = file_overlap_ratio * 0.5  # ä¿å®ˆä¼°ç®—
                            coverage_type = "limited_overlap"
            else:
                coverage = 0.0
                coverage_type = "no_overlap"
        
        return {
            'has_data_in_range': has_data,
            'estimated_coverage': coverage,
            'coverage_type': coverage_type,
            'sample_times': sample_times,
            'file_time_range': (file_start_time, file_end_time),
            'sample_count': len(sample_times)
        }
        
    except Exception as e:
        # Graceful fallback
        return {'has_data_in_range': True, 'estimated_coverage': 1.0, 'error': str(e)}

# ---------- Boundary scan ----------
def _find_all(mm, needle: bytes):
    pos = 0
    out = []
    L = len(needle)
    while True:
        i = mm.find(needle, pos)
        if i == -1:
            break
        out.append(i + (1 if needle.startswith(b"\n") else 0))  # point at '#'
        pos = i + L
    return out

def compute_boundaries(path: str, max_parts: int, loose_start: bool):
    """
    Lightweight boundary scan without time filtering.
    Time filtering is now handled by smart sampling and worker-level filtering.
    """
    with open(path, "rb") as f:
        mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
        try:
            starts = set()
            if loose_start:
                if mm[:7] == b"# Time:":
                    starts.add(0)
                if mm[:14] == b"# Query_time:":
                    starts.add(0)
                starts.update(_find_all(mm, b"\n# Time:"))
                starts.update(_find_all(mm, b"\n# Query_time:"))
            else:
                if mm[:7] == b"# Time:":
                    starts.add(0)
                starts.update(_find_all(mm, b"\n# Time:"))
            
            starts = sorted(starts)
            if not starts:
                return [(0, mm.size())], 0, mm.size()
            
            parts = min(max_parts, max(1, len(starts)))
            idxs = [int(round(i*len(starts)/parts)) for i in range(parts+1)]
            idxs = sorted(set(max(0, min(k, len(starts))) for k in idxs))
            
            shards = []
            for a,b in zip(idxs[:-1], idxs[1:]):
                if a==b: 
                    continue
                start = starts[a]
                end = starts[b] if b < len(starts) else mm.size()
                shards.append((start, end))
            
            if not shards:
                shards = [(0, mm.size())]
            return shards, len(starts), mm.size()
        finally:
            mm.close()

# ---------- Chunk parser ----------
def parse_chunk(args):
    """
    Enhanced chunk parser with early termination and memory awareness.
    Returns (agg_dict, stats_dict)
    """
    path, start, end, min_time, exclude_dumps, mark_truncated, loose_start, time_range = args
    with open(path, "rb") as fb:
        fb.seek(start)
        data = fb.read(end - start)
    text = data.decode("utf-8", errors="ignore").splitlines()

    stats = {
        "time_lines": 0,
        "qtime_lines": 0,
        "parsed_records": 0,
        "filtered_min_time": 0,
        "filtered_dumps": 0,
        "truncated_records": 0,
        "filtered_time_range": 0,
    }

    result = {}

    current = {
        "time": None, "user_host": None, "query_time": None, "lock_time": None,
        "rows_sent": None, "rows_examined": None, "bytes_sent": None, "bytes_received": None,
        "start": None, "end": None, "db": None, "set_timestamp": None, "thread_id": None, "errno": None,
    }
    sql_buf = []
    last_db = None
    started = False  # have we seen a record header?

    def add_entry(entry, truncated=False):
        nonlocal result, last_db
        sql = entry.get("sql","").strip()
        if not sql:
            return
        if exclude_dumps and "sql_no_cache" in sql.lower() and "/*!" in sql:
            stats["filtered_dumps"] += 1
            return
        qt = entry.get("query_time")
        if qt is None:
            return
        try:
            qt = float(qt)
        except:
            return
        if qt < min_time:
            stats["filtered_min_time"] += 1
            return
        
        # Apply time range filtering (optimized)
        if time_range:
            time_in_range = False
            # Check primary time source first (most reliable)
            time_val = entry.get("time")
            if time_val:
                dt = parse_mysql_time(str(time_val))
                if dt:
                    target_start, target_end = time_range
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    elif dt.tzinfo != timezone.utc:
                        dt = dt.astimezone(timezone.utc)
                    time_in_range = target_start <= dt <= target_end
            
            # Fallback to other time sources if needed
            if not time_in_range:
                for time_key in ("set_timestamp", "start", "end"):
                    time_val = entry.get(time_key)
                    if time_val:
                        dt = parse_mysql_time(str(time_val))
                        if dt:
                            target_start, target_end = time_range
                            if dt.tzinfo is None:
                                dt = dt.replace(tzinfo=timezone.utc)
                            elif dt.tzinfo != timezone.utc:
                                dt = dt.astimezone(timezone.utc)
                            if target_start <= dt <= target_end:
                                time_in_range = True
                                break
            
            if not time_in_range:
                stats["filtered_time_range"] += 1
                return

        if truncated and mark_truncated:
            sql = sql + " /* TRUNCATED */"

        fp = fingerprint(sql)
        g = result.get(fp)
        if g is None:
            g = {
                "samples": 0,
                "total_time_s": 0.0,
                "max_time_s": 0.0,
                "sum_lock_time_s": 0.0,
                "rows_examined_total": 0,
                "rows_sent_total": 0,
                "durations": [],
                "first_seen": None,
                "last_seen": None,
                "norm_sql": normalize_sql(sql),
                "example_query": sql[:1500],
                "db": entry.get("db"),
                "user_host": entry.get("user_host"),
                "main_table": extract_main_table(sql),
                "has_truncated": bool(truncated),
            }
            result[fp] = g
        else:
            if truncated:
                g["has_truncated"] = True

        g["samples"] += 1
        g["total_time_s"] += qt
        g["durations"].append(qt)
        if qt > g["max_time_s"]:
            g["max_time_s"] = qt
        lt = entry.get("lock_time")
        if lt is not None:
            try: g["sum_lock_time_s"] += float(lt)
            except: pass
        rex = entry.get("rows_examined")
        if rex is not None:
            try: g["rows_examined_total"] += int(rex)
            except: pass
        rs = entry.get("rows_sent")
        if rs is not None:
            try: g["rows_sent_total"] += int(rs)
            except: pass

        # update times by raw string compare
        for key in ("time","start","end","set_timestamp"):
            val = entry.get(key)
            if not val: continue
            if g["first_seen"] is None or str(val) < str(g["first_seen"]):
                g["first_seen"] = val
            if g["last_seen"] is None or str(val) > str(g["last_seen"]):
                g["last_seen"] = val

    def flush(truncated=False):
        nonlocal current, sql_buf, last_db, started
        sql = "\n".join(sql_buf).strip()
        if sql:
            row = current.copy()
            row["db"] = current["db"] or last_db
            row["sql"] = sql
            add_entry(row, truncated=truncated)
            stats["parsed_records"] += 1
        current = {k: None for k in current}
        sql_buf = []

    for l in text:
        if l.startswith("# Time:"):
            stats["time_lines"] += 1
            if sql_buf and started:
                flush(truncated=False)
            started = True
            current["time"] = l.split("# Time:",1)[1].strip()
            continue
        if l.startswith("# Query_time:"):
            stats["qtime_lines"] += 1
            # In loose mode, if we haven't seen a start yet, treat this as a header start
            if loose_start and not started:
                if sql_buf:
                    flush(truncated=False)
                started = True
            qt = re.search(r"Query_time:\s*([\d\.]+)", l)
            lt = re.search(r"Lock_time:\s*([\d\.]+)", l)
            rs = re.search(r"Rows_sent:\s*(\d+)", l)
            rexa = re.search(r"Rows_examined:\s*(\d+)", l)
            st = re.search(r"Start:\s*([^\s]+)", l)
            en = re.search(r"End:\s*([^\s]+)", l)
            if qt: current["query_time"] = float(qt.group(1))
            if lt: current["lock_time"] = float(lt.group(1))
            if rs: current["rows_sent"] = int(rs.group(1))
            if rexa: current["rows_examined"] = int(rexa.group(1))
            if st: current["start"] = st.group(1)
            if en: current["end"] = en.group(1)
            continue
        m_use = re.match(r"\s*use\s+([`\"\w\.\-]+);", l, flags=re.IGNORECASE)
        if m_use:
            dbname = m_use.group(1).strip("`\"")
            current["db"] = dbname
            last_db = dbname
            continue
        if l.startswith("SET timestamp="):
            m = re.search(r"SET timestamp=(\d+);", l)
            if m:
                current["set_timestamp"] = m.group(1)
            continue
        if l.startswith("# "):
            continue
        if not l.strip():
            continue
        sql_buf.append(l)

    # tail flush: if we were inside a record, mark as possibly truncated
    if sql_buf:
        stats["truncated_records"] += 1
        flush(truncated=True)

    return result, stats

# ---------- Merge worker results ----------
def merge_results(parts):
    agg = {}
    stats_total = {
        "time_lines": 0, "qtime_lines": 0, "parsed_records": 0,
        "filtered_min_time": 0, "filtered_dumps": 0, "truncated_records": 0, "filtered_time_range": 0,
    }
    for d, st in parts:
        for k in stats_total:
            stats_total[k] += st.get(k, 0)
        for fp, g in d.items():
            t = agg.get(fp)
            if t is None:
                agg[fp] = {
                    "fingerprint": fp,
                    "samples": g["samples"],
                    "total_time_s": g["total_time_s"],
                    "max_time_s": g["max_time_s"],
                    "sum_lock_time_s": g["sum_lock_time_s"],
                    "rows_examined_total": g["rows_examined_total"],
                    "rows_sent_total": g["rows_sent_total"],
                    "durations": list(g["durations"]),
                    "first_seen": g["first_seen"],
                    "last_seen": g["last_seen"],
                    "norm_sql": g["norm_sql"],
                    "example_query": g["example_query"],
                    "db": g["db"],
                    "user_host": g["user_host"],
                    "main_table": g["main_table"],
                    "has_truncated": g.get("has_truncated", False),
                }
            else:
                t["samples"] += g["samples"]
                t["total_time_s"] += g["total_time_s"]
                t["max_time_s"] = max(t["max_time_s"], g["max_time_s"])
                t["sum_lock_time_s"] += g["sum_lock_time_s"]
                t["rows_examined_total"] += g["rows_examined_total"]
                t["rows_sent_total"] += g["rows_sent_total"]
                t["durations"].extend(g["durations"])
                if g["first_seen"] and (t["first_seen"] is None or str(g["first_seen"]) < str(t["first_seen"])):
                    t["first_seen"] = g["first_seen"]
                if g["last_seen"] and (t["last_seen"] is None or str(g["last_seen"]) > str(t["last_seen"])):
                    t["last_seen"] = g["last_seen"]
                if g.get("has_truncated"):
                    t["has_truncated"] = True
                for k in ("db","user_host","main_table"):
                    if not t[k] and g[k]:
                        t[k] = g[k]
    return agg, stats_total

# ---------- Build DataFrame & outputs ----------
def build_dataframe(agg):
    rows = []
    for fp, g in agg.items():
        durations = np.array(g["durations"], dtype=float)
        avg_time = float(np.mean(durations)) if len(durations) else 0.0
        p95_time = float(np.percentile(durations, 95)) if len(durations) else np.nan
        avg_lock = g["sum_lock_time_s"]/g["samples"] if g["samples"] else np.nan
        rows_examined_avg = g["rows_examined_total"]/g["samples"] if g["samples"] else np.nan
        rows_sent_avg = g["rows_sent_total"]/g["samples"] if g["samples"] else np.nan
        rows.append({
            "fingerprint": fp,
            "samples": g["samples"],
            "total_time_s": g["total_time_s"],
            "avg_time_s": avg_time,
            "p95_time_s": p95_time,
            "max_time_s": g["max_time_s"],
            "avg_lock_time_s": avg_lock,
            "rows_examined_total": g["rows_examined_total"],
            "rows_examined_avg": rows_examined_avg,
            "rows_sent_total": g["rows_sent_total"],
            "rows_sent_avg": rows_sent_avg,
            "first_seen": g["first_seen"],
            "last_seen": g["last_seen"],
            "example_query": g["example_query"],
            "norm_sql": g["norm_sql"],
            "db": g["db"],
            "user_host": g["user_host"],
            "main_table": g["main_table"],
            "has_truncated": g.get("has_truncated", False),
        })
    df = pd.DataFrame(rows)
    if not df.empty:
        total_time = df["total_time_s"].sum()
        total_count = df["samples"].sum()
        df["time_share_pct"]  = (df["total_time_s"]/total_time*100.0).round(3)
        df["count_share_pct"] = (df["samples"]/total_count*100.0).round(3)
        df.sort_values(["total_time_s","samples"], ascending=[False, False], inplace=True)
    return df

def rename_columns(df: "pd.DataFrame", lang: str) -> "pd.DataFrame":
    if df is None or df.empty:
        return df
    if lang.lower() not in ("zh", "en"):
        lang = "zh"
    if lang.lower() == "en":
        return df
    col_map = {
        "fingerprint": "æŒ‡çº¹",
        "samples": "æ ·æœ¬æ•°",
        "total_time_s": "æ€»è€—æ—¶(s)",
        "avg_time_s": "å¹³å‡è€—æ—¶(s)",
        "p95_time_s": "P95è€—æ—¶(s)",
        "max_time_s": "æœ€å¤§è€—æ—¶(s)",
        "time_share_pct": "æ€»è€—æ—¶å æ¯”(%)",
        "count_share_pct": "æ¬¡æ•°å æ¯”(%)",
        "avg_lock_time_s": "å¹³å‡é”ç­‰å¾…(s)",
        "rows_examined_total": "æ‰«æè¡Œæ•°-æ€»è®¡",
        "rows_examined_avg": "æ‰«æè¡Œæ•°-å¹³å‡",
        "rows_sent_total": "è¿”å›è¡Œæ•°-æ€»è®¡",
        "rows_sent_avg": "è¿”å›è¡Œæ•°-å¹³å‡",
        "db": "æ•°æ®åº“",
        "main_table": "ä¸»è¡¨",
        "user_host": "ç”¨æˆ·@ä¸»æœº",
        "norm_sql": "è§„èŒƒåŒ–SQL",
        "example_query": "ç¤ºä¾‹SQL",
        "first_seen": "é¦–æ¬¡å‡ºç°æ—¶é—´",
        "last_seen": "æœ€åå‡ºç°æ—¶é—´",
        "has_truncated": "å«æˆªæ–­æ ·æœ¬",
    }
    out = df.rename(columns={k:v for k,v in col_map.items() if k in df.columns}).copy()
    order = ["æŒ‡çº¹","æ ·æœ¬æ•°","æ€»è€—æ—¶(s)","å¹³å‡è€—æ—¶(s)","P95è€—æ—¶(s)","æœ€å¤§è€—æ—¶(s)","æ€»è€—æ—¶å æ¯”(%)","æ¬¡æ•°å æ¯”(%)",
             "å¹³å‡é”ç­‰å¾…(s)","æ‰«æè¡Œæ•°-æ€»è®¡","æ‰«æè¡Œæ•°-å¹³å‡","è¿”å›è¡Œæ•°-æ€»è®¡","è¿”å›è¡Œæ•°-å¹³å‡",
             "æ•°æ®åº“","ä¸»è¡¨","ç”¨æˆ·@ä¸»æœº","è§„èŒƒåŒ–SQL","ç¤ºä¾‹SQL","é¦–æ¬¡å‡ºç°æ—¶é—´","æœ€åå‡ºç°æ—¶é—´","å«æˆªæ–­æ ·æœ¬"]
    cols = [c for c in order if c in out.columns] + [c for c in out.columns if c not in order]
    return out[cols]

def write_markdown(df, out_md, top):
    if df is None or df.empty:
        with open(out_md, "w", encoding="utf-8") as f:
            f.write("# MySQL æ…¢æ—¥å¿—æ±‡æ€»\n\nï¼ˆæ— æ•°æ®ï¼‰\n")
        return
    topN = min(top, len(df))
    lines = []
    lines.append(f"# MySQL æ…¢æ—¥å¿—æ±‡æ€»ï¼ˆTop {topN} æŒ‰æ€»è€—æ—¶ï¼‰\n")
    lines.append(f"- æ€»æ ·æœ¬æ•°ï¼š**{int(df['samples'].sum())}**\n")
    lines.append(f"- æ€»è€—æ—¶ï¼š**{df['total_time_s'].sum():.3f} s**\n")
    lines.append("| æ’å | æ ·æœ¬æ•° | æ€»è€—æ—¶(s) | å¹³å‡è€—æ—¶(s) | P95è€—æ—¶(s) | æœ€å¤§è€—æ—¶(s) | æ€»è€—æ—¶å æ¯”(%) | ä¸»è¡¨ | æ•°æ®åº“ | æŒ‡çº¹ | è§„èŒƒåŒ–SQL(å‰120å­—) |")
    lines.append("|---:|---:|---:|---:|---:|---:|---:|---|---|---|---|")
    for i, row in df.head(topN).reset_index(drop=True).iterrows():
        norm_short = (str(row.get("norm_sql",""))[:120]).replace("|","\\|")
        p95v = "" if (row.get("p95_time_s") is None or pd.isna(row.get("p95_time_s"))) else f"{float(row['p95_time_s']):.3f}"
        lines.append("| {} | {} | {:.3f} | {:.3f} | {} | {:.3f} | {} | {} | {} | `{}` | {} |".format(
            i+1, int(row["samples"]), float(row["total_time_s"]), float(row["avg_time_s"]),
            p95v, float(row["max_time_s"]), row.get("time_share_pct",""), row.get("main_table","") or "",
            row.get("db","") or "", row["fingerprint"], norm_short
        ))
    with open(out_md, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

# ---------- Elasticsearch Integration ----------
def create_es_client(es_hosts, es_user=None, es_password=None, es_ca_certs=None, es_verify_certs=True):
    """
    åˆ›å»ºElasticsearchå®¢æˆ·ç«¯è¿æ¥
    """
    if not ES_AVAILABLE:
        raise ImportError("elasticsearch package not available. Install with: pip install elasticsearch")
    
    try:
        # æ„å»ºè¿æ¥é…ç½®
        config = {
            'hosts': es_hosts if isinstance(es_hosts, list) else [es_hosts],
            'verify_certs': es_verify_certs,
            'request_timeout': 30,
            'retry_on_timeout': True,
            'max_retries': 3
        }
        
        # æ·»åŠ è®¤è¯ï¼ˆä½¿ç”¨æ–°çš„APIï¼‰
        if es_user and es_password:
            config['basic_auth'] = (es_user, es_password)
        
        # æ·»åŠ CAè¯ä¹¦
        if es_ca_certs:
            config['ca_certs'] = es_ca_certs
        
        client = Elasticsearch(**config)
        
        # æµ‹è¯•è¿æ¥
        if not client.ping():
            raise ConnectionError("æ— æ³•è¿æ¥åˆ°Elasticsearché›†ç¾¤")
        
        return client
    
    except Exception as e:
        print(f"âš ï¸ Elasticsearchè¿æ¥å¤±è´¥: {e}")
        return None

def prepare_es_documents(df, index_pattern="mysql-slowlog-%{+yyyy.MM.dd}", hostname=None, log_file_path=None):
    """
    å°†DataFrameè½¬æ¢ä¸ºElasticsearchæ–‡æ¡£æ ¼å¼
    
    éµå¾ªECS (Elastic Common Schema) æ ‡å‡†ï¼š
    - host.*: ä¸»æœºå…ƒæ•°æ®ï¼ˆåç§°ã€IPã€æ“ä½œç³»ç»Ÿï¼‰
    - log.file.*: æ—¥å¿—æ–‡ä»¶ä¿¡æ¯ï¼ˆè·¯å¾„ã€åç§°ã€ç›®å½•ï¼‰
    - agent.*: é‡‡é›†å™¨ä¿¡æ¯ï¼ˆåç§°ã€ç‰ˆæœ¬ã€ç±»å‹ï¼‰
    - service.*: æœåŠ¡æ ‡è¯†ï¼ˆMySQLæ•°æ®åº“ï¼‰
    - mysql.slowlog.*: MySQLæ…¢æ—¥å¿—ä¸“ç”¨å­—æ®µ
    
    ä¼˜åŠ¿ï¼š
    - ç¬¦åˆElastic Stackç”Ÿæ€æ ‡å‡†
    - æ— é‡å¤å­—æ®µï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´
    - ä¾¿äºè·¨æ•°æ®æºå…³è”åˆ†æ
    - æ”¯æŒæ ‡å‡†åŒ–å¯è§†åŒ–æ¨¡æ¿
    """
    if df is None or df.empty:
        return []
    
    documents = []
    current_time = datetime.now(timezone.utc)
    
    # è§£æç´¢å¼•æ¨¡å¼ä¸­çš„æ—¥æœŸ
    index_name = current_time.strftime(index_pattern.replace('%{+yyyy.MM.dd}', '%Y.%m.%d'))
    
    # è·å–ç³»ç»Ÿä¿¡æ¯
    import socket
    import platform
    
    # ä¸»æœºä¿¡æ¯
    system_hostname = hostname or socket.gethostname()
    system_ip = socket.getfqdn()
    system_os = platform.system()
    system_arch = platform.machine()
    
    # æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼ˆè·¨å¹³å°å…¼å®¹ï¼‰
    if log_file_path:
        file_path = os.path.abspath(log_file_path).replace('\\', '/')  # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ 
        file_name = os.path.basename(log_file_path)
        file_dir = os.path.dirname(file_path)
    else:
        file_path = 'unknown'
        file_name = 'unknown'
        file_dir = 'unknown'
    
    for _, row in df.iterrows():
        # æ„å»ºESæ–‡æ¡£
        doc = {
            '@timestamp': current_time.isoformat(),
            'analysis_date': current_time.strftime('%Y-%m-%d'),
            
            # ========== æ•°æ®æºæ ‡è¯†ï¼ˆç±»ä¼¼filebeatï¼‰ ==========
            'host': {
                'name': system_hostname,
                'ip': system_ip,
                'os': {
                    'family': system_os,
                    'platform': system_os.lower(),
                    'architecture': system_arch
                }
            },
            'log': {
                'file': {
                    'path': file_path,
                    'name': file_name,
                    'directory': file_dir
                }
            },
            'agent': {
                'name': 'mysql_slowlog_analyzer',
                'version': 'expert',
                'type': 'mysql_analyzer'
            },
            
            # ========== ä¸šåŠ¡æ ‡è¯† ==========
            'service': {
                'name': 'mysql',
                'type': 'database'
            },
            'mysql': {
                'slowlog': {
                    'fingerprint': row.get('fingerprint', ''),
                    'samples': int(row.get('samples', 0)),
                    
                    # æ€§èƒ½æŒ‡æ ‡
                    'query_time': {
                        'total_seconds': float(row.get('total_time_s', 0)),
                        'avg_seconds': float(row.get('avg_time_s', 0)),
                        'p95_seconds': float(row.get('p95_time_s', 0)) if pd.notna(row.get('p95_time_s')) else None,
                        'max_seconds': float(row.get('max_time_s', 0))
                    },
                    
                    # å æ¯”ä¿¡æ¯
                    'share': {
                        'time_percent': float(row.get('time_share_pct', 0)),
                        'count_percent': float(row.get('count_share_pct', 0))
                    },
                    
                    # é”å’Œè¡Œæ•°ç»Ÿè®¡
                    'lock_time': {
                        'avg_seconds': float(row.get('avg_lock_time_s', 0)) if pd.notna(row.get('avg_lock_time_s')) else 0
                    },
                    'rows': {
                        'examined_total': int(row.get('rows_examined_total', 0)),
                        'examined_avg': float(row.get('rows_examined_avg', 0)) if pd.notna(row.get('rows_examined_avg')) else 0,
                        'sent_total': int(row.get('rows_sent_total', 0)),
                        'sent_avg': float(row.get('rows_sent_avg', 0)) if pd.notna(row.get('rows_sent_avg')) else 0
                    },
                    
                    # ç»´åº¦ä¿¡æ¯
                    'database': row.get('db', '') or '',
                    'table': row.get('main_table', '') or '',
                    'user_host': row.get('user_host', '') or '',
                    
                    # SQLä¿¡æ¯
                    'sql': {
                        'normalized': row.get('norm_sql', ''),
                        'example': row.get('example_query', '') or '',  # å®Œæ•´æ˜¾ç¤ºï¼Œä¸é™åˆ¶é•¿åº¦
                        'has_truncated': bool(row.get('has_truncated', False))
                    },
                    
                    # æ—¶é—´ä¿¡æ¯
                    'time_range': {
                        'first_seen': row.get('first_seen', ''),
                        'last_seen': row.get('last_seen', '')
                    }
                }
            },
            
            # ========== ECSæ ‡å‡†ç»“æ„ï¼Œæ— é‡å¤å­—æ®µ ==========
            # æ‰€æœ‰æ•°æ®é€šè¿‡æ ‡å‡†ECSå­—æ®µè®¿é—®ï¼š
            # - ä¸»æœºä¿¡æ¯: host.*
            # - æ—¥å¿—æ–‡ä»¶: log.file.*
            # - MySQLæ•°æ®: mysql.slowlog.*
            # - æœåŠ¡ä¿¡æ¯: service.*
            # - é‡‡é›†å™¨: agent.*
        }
        
        documents.append({
            '_index': index_name,
            '_source': doc
        })
    
    return documents

def send_to_elasticsearch(client, documents, chunk_size=100):
    """
    æ‰¹é‡å‘é€æ–‡æ¡£åˆ°Elasticsearch
    """
    if not documents:
        return {'success': 0, 'failed': 0}
    
    try:
        # ä½¿ç”¨bulk APIæ‰¹é‡ç´¢å¼•
        success_count = 0
        failed_count = 0
        
        for i in range(0, len(documents), chunk_size):
            chunk = documents[i:i + chunk_size]
            try:
                response = bulk(client, chunk, refresh=True)
                success_count += len(chunk)
            except Exception as e:
                print(f"âš ï¸ æ‰¹é‡ç´¢å¼•å¤±è´¥ (chunk {i//chunk_size + 1}): {e}")
                failed_count += len(chunk)
        
        return {'success': success_count, 'failed': failed_count}
    
    except Exception as e:
        print(f"âš ï¸ Elasticsearchç´¢å¼•å¤±è´¥: {e}")
        return {'success': 0, 'failed': len(documents)}

def main():
    # è·¨å¹³å°å¤šè¿›ç¨‹å…¼å®¹æ€§ä¿®å¤
    import multiprocessing
    import platform
    
    # æ ¹æ®æ“ä½œç³»ç»Ÿè®¾ç½®åˆé€‚çš„å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    if platform.system() == 'Windows':
        # Windowsä¸‹ä½¿ç”¨spawnæ–¹æ³•é¿å…KeyboardInterrupté—®é¢˜
        if hasattr(multiprocessing, 'set_start_method'):
            try:
                multiprocessing.set_start_method('spawn', force=True)
            except RuntimeError:
                pass  # å·²ç»è®¾ç½®è¿‡äº†
    else:
        # Linux/macOSä¸‹ä½¿ç”¨forkæ–¹æ³•ï¼ˆæ€§èƒ½æ›´å¥½ï¼‰
        if hasattr(multiprocessing, 'set_start_method'):
            try:
                multiprocessing.set_start_method('fork', force=True)
            except (RuntimeError, OSError):
                # å¦‚æœforkä¸å¯ç”¨ï¼Œå›é€€åˆ°spawn
                try:
                    multiprocessing.set_start_method('spawn', force=True)
                except RuntimeError:
                    pass
    
    ap = argparse.ArgumentParser(description="Parallel MySQL slow log aggregator (robust v2)")
    ap.add_argument("logfile", help="path to MySQL slow log")
    ap.add_argument("--out-csv", default="slowlog_summary.csv")
    ap.add_argument("--out-md", default=None)
    ap.add_argument("--top", type=int, default=10)
    ap.add_argument("--lang", default="zh", choices=["zh","en"])
    ap.add_argument("--min-time", type=float, default=0.0)
    ap.add_argument("--exclude-dumps", action="store_true")
    ap.add_argument("--jobs", type=int, default=cpu_count(), help="workers (default = CPU cores)")
    ap.add_argument("--loose-start", action="store_true", help="treat '# Query_time:' as a valid start when '# Time:' missing")
    ap.add_argument("--mark-truncated", action="store_true", help="append /* TRUNCATED */ to example/norm SQL when tail-truncated")
    ap.add_argument("--stats", action="store_true", help="print processing stats and timings")
    
    # Time filtering options
    time_group = ap.add_mutually_exclusive_group()
    time_group.add_argument("--days", type=int, help="analyze last N days (0=today only, 1=last 1 day, 7=last 7 days, etc.)")
    time_group.add_argument("--today", action="store_true", help="analyze today only (same as --days 0)")
    time_group.add_argument("--all", action="store_true", help="analyze all records (default, no time filtering)")
    
    # Elasticsearch integration options
    es_group = ap.add_argument_group('Elasticsearch Integration')
    es_group.add_argument("--es-host", help="Elasticsearch host (e.g., http://localhost:9200)")
    es_group.add_argument("--es-index", default="mysql-slowlog-%{+yyyy.MM.dd}", 
                         help="Elasticsearch index pattern (default: mysql-slowlog-%%{+yyyy.MM.dd})")
    es_group.add_argument("--es-user", help="Elasticsearch username")
    es_group.add_argument("--es-password", help="Elasticsearch password")
    es_group.add_argument("--es-ca-certs", help="Path to CA certificate file")
    es_group.add_argument("--es-no-verify-certs", action="store_true", help="Disable SSL certificate verification")
    es_group.add_argument("--es-hostname", help="Hostname to include in ES documents (default: system hostname)")
    
    args = ap.parse_args()
    
    # Elasticsearch configuration validation
    if args.es_host:
        print("ğŸ“¡ æ£€æµ‹åˆ°Elasticsearché…ç½®...")
        
        # Check if elasticsearch package is available
        if not ES_AVAILABLE:
            print("âŒ é”™è¯¯: ElasticsearchåŠŸèƒ½éœ€è¦å®‰è£…elasticsearchåŒ…")
            print("   å®‰è£…å‘½ä»¤: pip install elasticsearch")
            print("   æˆ–è€…: pip install elasticsearch[async]")
            sys.exit(1)
        
        # Configuration recommendations
        print("ğŸ”§ Elasticsearché…ç½®æ£€æŸ¥:")
        print(f"   è¿æ¥åœ°å€: {args.es_host}")
        print(f"   ç´¢å¼•æ¨¡å¼: {args.es_index}")
        
        # Check authentication
        if args.es_user and args.es_password:
            print(f"   è®¤è¯ç”¨æˆ·: {args.es_user}")
            print("   è®¤è¯å¯†ç : *** (å·²é…ç½®)")
        else:
            print("   è®¤è¯æ–¹å¼: æ— è®¤è¯")
            if "https://" in args.es_host.lower():
                print("   âš ï¸  å»ºè®®: HTTPSè¿æ¥é€šå¸¸éœ€è¦é…ç½®ç”¨æˆ·åå¯†ç ")
                print("   ä½¿ç”¨: --es-user <ç”¨æˆ·å> --es-password <å¯†ç >")
        
        # Check SSL settings
        if args.es_ca_certs:
            print(f"   SSLè¯ä¹¦: {args.es_ca_certs}")
        elif "https://" in args.es_host.lower():
            print("   SSLéªŒè¯: é»˜è®¤å¼€å¯")
            if args.es_no_verify_certs:
                print("   âš ï¸  è­¦å‘Š: SSLè¯ä¹¦éªŒè¯å·²ç¦ç”¨")
            else:
                print("   ğŸ’¡ æç¤º: å¦‚é‡SSLè¯ä¹¦é—®é¢˜ï¼Œå¯ä½¿ç”¨ --es-no-verify-certs")
        
        # Hostname for documents
        hostname = args.es_hostname or (os.uname().nodename if hasattr(os, 'uname') else 'unknown')
        print(f"   æ–‡æ¡£ä¸»æœº: {hostname}")
        
        print()
    
    # Handle time filtering arguments
    time_range = None
    if args.today or args.days == 0:
        time_range = calculate_time_range(0)
        print(f"æ—¶é—´è¿‡æ»¤: ä»…ä»Šå¤© ({time_range[0].strftime('%Y-%m-%d')})")
    elif args.days is not None and args.days > 0:
        time_range = calculate_time_range(args.days)
        print(f"æ—¶é—´è¿‡æ»¤: æœ€è¿‘ {args.days} å¤© ({time_range[0].strftime('%Y-%m-%d')} è‡³ {time_range[1].strftime('%Y-%m-%d')})")
    else:
        print("æ—¶é—´è¿‡æ»¤: å…¨éƒ¨è®°å½• (æ— è¿‡æ»¤)")

    t0 = time.perf_counter()
    
    # æ™ºèƒ½æ—¶é—´èŒƒå›´é¢„æ£€æŸ¥ï¼ˆä¸“å®¶çº§ä¼˜åŒ–ï¼‰
    if time_range:
        print("ğŸ” æ™ºèƒ½é‡‡æ ·æ£€æŸ¥æ—¶é—´èŒƒå›´...")
        sample_result = smart_time_range_check(args.logfile, time_range)
        
        if not sample_result['has_data_in_range']:
            print(f"âš ï¸  åŸºäºé‡‡æ ·åˆ†æï¼ŒæŒ‡å®šæ—¶é—´èŒƒå›´å†…æœªå‘ç°æ•°æ®")
            if 'file_time_range' in sample_result and sample_result['file_time_range'][0]:
                file_start, file_end = sample_result['file_time_range']
                print(f"   æ–‡ä»¶æ—¶é—´èŒƒå›´: {file_start.strftime('%Y-%m-%d %H:%M')} è‡³ {file_end.strftime('%Y-%m-%d %H:%M')}")
            target_start, target_end = time_range
            print(f"   ç›®æ ‡æ—¶é—´èŒƒå›´: {target_start.strftime('%Y-%m-%d %H:%M')} è‡³ {target_end.strftime('%Y-%m-%d %H:%M')}")
            print(f"   å·²é‡‡æ · {sample_result.get('sample_count', 0)} ä¸ªæ—¶é—´æˆ³")
            return
        
        coverage = sample_result['estimated_coverage']
        coverage_type = sample_result.get('coverage_type', 'unknown')
        
        # æ ¹æ®è¦†ç›–ç‡ç±»å‹ç»™å‡ºæ›´å‡†ç¡®çš„æç¤º
        if coverage_type == "full_file_in_range":
            print(f"âœ… æ–‡ä»¶æ•°æ®å®Œå…¨åœ¨ç›®æ ‡æ—¶é—´èŒƒå›´å†…ï¼ˆè¦†ç›–ç‡: 100%ï¼‰")
        elif coverage_type == "full_range_covered":
            print(f"âœ… ç›®æ ‡æ—¶é—´èŒƒå›´å®Œå…¨è¢«æ–‡ä»¶è¦†ç›–ï¼ˆè¦†ç›–ç‡: 100%ï¼‰")
        elif coverage_type == "mostly_covered":
            print(f"âœ… åœ¨æ—¶é—´èŒƒå›´å†…å‘ç°å……è¶³æ•°æ®ï¼ˆé¢„ä¼°è¦†ç›–ç‡: {coverage*100:.0f}%ï¼‰")
        elif coverage_type == "partially_covered":
            print(f"âœ… åœ¨æ—¶é—´èŒƒå›´å†…å‘ç°éƒ¨åˆ†æ•°æ®ï¼ˆé¢„ä¼°è¦†ç›–ç‡: {coverage*100:.0f}%ï¼‰")
        else:
            print(f"âœ… åœ¨æ—¶é—´èŒƒå›´å†…å‘ç°æ•°æ®ï¼ˆé¢„ä¼°è¦†ç›–ç‡: {coverage*100:.1f}%ï¼‰")
            if coverage < 0.3:
                print(f"ğŸ’¡ æç¤º: æ–‡ä»¶æ•°æ®æ—¶é—´è·¨åº¦è¾ƒå°ï¼Œå¤§éƒ¨åˆ†ç›®æ ‡æ—¶é—´èŒƒå›´å¯èƒ½æ— æ•°æ®")
    
    # Lightweight boundary scan (no time filtering at this stage)
    shards, record_starts, file_size = compute_boundaries(args.logfile, args.jobs, args.loose_start)
    t_scan = time.perf_counter()

    job_args = [(args.logfile, s, e, args.min_time, args.exclude_dumps, args.mark_truncated, args.loose_start, time_range) for (s,e) in shards]
    with Pool(processes=min(args.jobs, len(job_args))) as pool:
        parts = pool.map(parse_chunk, job_args)
    t_parse = time.perf_counter()

    merged, stats_total = merge_results(parts)
    t_merge = time.perf_counter()

    # æ£€æŸ¥è¿‡æ»¤åæ˜¯å¦æœ‰æ•°æ®
    if not merged or len(merged) == 0:
        print("\nâš ï¸  åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å’Œæ¡ä»¶ä¸‹æœªæ‰¾åˆ°æ…¢æŸ¥è¯¢")
        if time_range:
            start_date = time_range[0].strftime('%Y-%m-%d')
            end_date = time_range[1].strftime('%Y-%m-%d')
            print(f"   æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        print(f"   æœ€å°è€—æ—¶é˜ˆå€¼: {args.min_time}ç§’")
        if args.stats:
            print(f"\n[ç»Ÿè®¡ä¿¡æ¯] ==========")
            print(f"æ–‡ä»¶å¤§å°         : {file_size} bytes")
            print(f"è®°å½•èµ·å§‹ç‚¹       : {record_starts}")
            print(f"è§£æè®°å½•æ•°       : {stats_total.get('parsed_records',0)}")
            print(f"è¿‡æ»¤<æœ€å°è€—æ—¶    : {stats_total.get('filtered_min_time',0)}")
            print(f"è¿‡æ»¤dumpsæŸ¥è¯¢    : {stats_total.get('filtered_dumps',0)}")
            print(f"è¿‡æ»¤æ—¶é—´èŒƒå›´     : {stats_total.get('filtered_time_range',0)}")
            print(f"æœ€ç»ˆå‰©ä½™         : 0")
        return

    df = build_dataframe(merged)
    t_build = time.perf_counter()

    out_df = rename_columns(df, args.lang)
    out_df.to_csv(args.out_csv, index=False, encoding="utf-8")
    print("å·²ä¿å­˜ CSV:", args.out_csv)
    if args.out_md:
        write_markdown(df, args.out_md, args.top)
        print("å·²ä¿å­˜ Markdown:", args.out_md)
    
    # Elasticsearch integration
    es_result = None
    if args.es_host:
        print("ğŸ“¡ æ­£åœ¨å‘é€æ•°æ®åˆ°Elasticsearch...")
        try:
            # åˆ›å»ºESå®¢æˆ·ç«¯
            es_client = create_es_client(
                es_hosts=args.es_host,
                es_user=args.es_user,
                es_password=args.es_password,
                es_ca_certs=args.es_ca_certs,
                es_verify_certs=not args.es_no_verify_certs
            )
            
            if es_client:
                # å‡†å¤‡æ–‡æ¡£
                hostname = args.es_hostname or (os.uname().nodename if hasattr(os, 'uname') else 'unknown')
                documents = prepare_es_documents(df, args.es_index, hostname, args.logfile)
                
                if documents:
                    # å‘é€åˆ°ES
                    es_result = send_to_elasticsearch(es_client, documents)
                    
                    if es_result['success'] > 0:
                        index_name = documents[0]['_index']
                        print(f"âœ… æˆåŠŸå‘é€ {es_result['success']} æ¡è®°å½•åˆ° Elasticsearch")
                        print(f"   ç´¢å¼•: {index_name}")
                        print(f"   ä¸»æœº: {hostname}")
                        print(f"   æ–‡ä»¶: {args.logfile}")
                        print(f"   æ—¶é—´æˆ³: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    
                    if es_result['failed'] > 0:
                        print(f"âš ï¸ {es_result['failed']} æ¡è®°å½•å‘é€å¤±è´¥")
                else:
                    print("âš ï¸ æ— æ•°æ®å‘é€åˆ°Elasticsearch")
            
        except Exception as e:
            print(f"âš ï¸ Elasticsearché›†æˆå¤±è´¥: {e}")
            print("   ğŸ’¡ è¯·æ£€æŸ¥:")
            print("     - ESæœåŠ¡æ˜¯å¦è¿è¡Œ")
            print("     - ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            print("     - ç”¨æˆ·åå¯†ç æ˜¯å¦æ­£ç¡®")
            print("     - SSLè¯ä¹¦é…ç½®æ˜¯å¦æ­£ç¡®")
    
    t_write = time.perf_counter()

    if args.stats:
        total_time = t_write - t0
        print("\n[ç»Ÿè®¡ä¿¡æ¯] ==========")
        print(f"æ–‡ä»¶å¤§å°         : {file_size} bytes")
        print(f"è®°å½•èµ·å§‹ç‚¹       : {record_starts}")
        print(f"åˆ†ç‰‡æ•°           : {len(shards)}; ä½¿ç”¨å·¥ä½œè¿›ç¨‹: {min(args.jobs, len(job_args))}")
        print(f"Time è¡Œæ•°        : {stats_total.get('time_lines',0)}")
        print(f"Query_time è¡Œæ•°  : {stats_total.get('qtime_lines',0)}")
        print(f"è§£æè®°å½•æ•°       : {stats_total.get('parsed_records',0)}")
        print(f"è¿‡æ»¤<æœ€å°è€—æ—¶    : {stats_total.get('filtered_min_time',0)}")
        print(f"è¿‡æ»¤dumpsæŸ¥è¯¢    : {stats_total.get('filtered_dumps',0)}")
        print(f"è¿‡æ»¤æ—¶é—´èŒƒå›´     : {stats_total.get('filtered_time_range',0)}")
        print(f"å°¾éƒ¨æˆªæ–­         : {stats_total.get('truncated_records',0)}")
        if df is not None and not df.empty:
            print(f"æŒ‡çº¹æ•°é‡         : {len(df)}")
            print(f"æ ·æœ¬æ€»æ•°         : {int(df['samples'].sum())}")
            print(f"æ€»è€—æ—¶ (ç§’)      : {float(df['total_time_s'].sum()):.3f}")
        
        # ESç»Ÿè®¡ä¿¡æ¯
        if es_result:
            print(f"ESå‘é€æˆåŠŸ       : {es_result['success']}")
            print(f"ESå‘é€å¤±è´¥       : {es_result['failed']}")
        
        print("[è€—æ—¶ç»Ÿè®¡] ==========")
        print(f"è¾¹ç•Œæ‰«æ         : {(t_scan - t0)*1000:.1f} ms")
        print(f"å¹¶è¡Œè§£æ         : {(t_parse - t_scan):.3f} s")
        print(f"ç»“æœåˆå¹¶         : {(t_merge - t_parse)*1000:.1f} ms")
        print(f"æ„å»ºDataFrame    : {(t_build - t_merge)*1000:.1f} ms")
        print(f"å†™å‡ºæ–‡ä»¶         : {(t_write - t_build)*1000:.1f} ms")
        print(f"æ€»è®¡             : {total_time:.3f} s")

if __name__ == "__main__":
    # è¿™ä¸ªä¿æŠ¤å¯¹Windowså¤šè¿›ç¨‹éå¸¸é‡è¦
    main()
